detectMarkers => returns:
- markerCorners: a list of all the corners of all the markers it found
- markerIds : a list of all IDs of all the markers it found
estimatePoseSingleMarker: will estimate the 3D pose of all the markers
=> returns:
- rvec: list of rotation vectors, one vector for each marker it found
- tvec: list of transalation vectors, one vector for each marker it found
https://media5.datahacker.rs/2019/04/28-1024x577.png
http://ycpcs.github.io/cs470-fall2014/labs/images/lab07/viewSystem.png
=> tvec (OpenCV) tells u in camera frame (refer to picture):
how far right (X), how far down (Y), how far forward (Z)
to go in order to get from the center of the camera frame to the center of the marker
=> tvec tells you the pose of each marker's center in the camera frame/world
=> once u get there, rvec tells you how much to rotate in 3D to line yourself
perfectly square with the marker's center (you become the marker)
at that point, you have the marker's frame:
X is the right axis (red)
Y is the forward axis (green)
Z is the up axis (blue)

let's say point P=Target1's center.
since we have the marker center's coordinates (point O):
in marker's frame: x=0, y=0 ,z=0
in camera frame: tvec

so the coordinates of P are:
in the marker frame is: P_x_marker=10cm, P_y_marker= -3.75cm, P_z_marker=0
in camera frame: ?? (this is what we need to determine)

intuition:
if we want to go from marker (or any point in its frame)
back to the camera, we need to rotate around the same axis (refer to picture) but in
the opposite direction (so first we need to flip rotation matrix),
and then translate backwards (flip tvec)
1/ get the rotation matrix via rodrigues:
Mat rot_mat;
Rodrigues(rvec, rmat);

2/ Flip/transpose rmat:
Mat rmat = rmat.t();

3/ Multiply the rotation matrix by the desired point (in the marker's frame)

4/ add the translation vector in order to translate backwards

summary: to go from a point P in the marker's frame into the camera's frame
[P_x_cam]                          [P_x_marker]
[P_y_cam] = rmat[marker's index] * [P_y_marker] + tvec[marker's index]
[P_z_cam]                          [P_z_marker]    
_______________________
0/ P=target1's center
1/ Mat rmat = new Mat();
   Calib3d.Rodrigues(rvec[marker's index], rmat);
Mat rmat = rmat.t();
2/ convert rotation matrix to array double [][]

3/ 
[P_x_cam          [[ 5  , 2 , 3 ]            [[ 10cm  ]         [ 3   ]
,P_y_cam    =      [ 1  , 4 , 3 ]    *        [-3.75cm]     +   [ 4   ]
,P_z_cam]          [ 0  , 3 , 2 ]]            [ 0     ]]        [ 1   ]

 
3x1          =         3x3           *          3x1         +    3x1

     Point3f camWorldE(rmat[0][0]*10,
                       rmat[0][1]*10,
                       rmat[0][2]*10);
     Point3f camWorldF(rmat[1][0]*-3.75,
                       rmat[1][1]*-3.75,
                       rmat[1][2]*-3.75);
     Point3f tvec_3f  (tvec[0],
		       tvec[1],
		       tvec[2]);
Point3f target = new Point3f
target = tvec_3f+  camWorldE + camWorldF;
(x       = (rmat[0][0]*10+rmat[1][0]*-3.75+rmat[2][0]*0+tvec[0],
 y          rmat[0][1]*10+rmat[1][1]*-3.75+rmat[2][1]*0+tvec[1],
 z)         rmat[0][2]*10+rmat[1][2]*-3.75+rmat[2][2]*0+tvec[2])
 
=> found the target's coordinates in the camera frame
=> how to align the laser pointer in order to shoot the target perfectly in the center?

PROBLEM: real world coordinates <> camera frame coordinates <> robot frame coordinates
difference in orientation and translation, scale is the same

Point1  (10.71000, -7.70000, 4.48000)
target2 (11.27460, -9.92284, 5.29881)
middle 1 (11.4,       -8.5,     4.7)


//tvec & rvec estimateBoard: what do they mean?
>> or change to estimateSingleMarker (ID N°2 or ID N°13)




_________________Image processing algorithm (after reporting Point1 arrival)_________________
1) Find the position of the laser pointer
2) Find the position of every aruco marker in the target plan
You can find the position of each aruco markers in the camera frame
3) Frame transformation:  =>
the camera pose in each marker’s coordinate system => reverse
=> markers in the camera’s coordinate system
=> transform everything to the baseline’s coordinate system (target)

4) Calculate the target position (target 1: no img processing)
(target 2: img processing)
5) Align the laser to the target
getMatNavcam image
saveMatImage
analyze image

LOOPCOUNTER=0
while (!aligned  & LOOP_MAX=5)
	reposition: based on the camera pose related to the target
	getMatNavcam image
	saveMatImage
	analyze image
	LOOPCOUNTER++
irradiate the laser
take snapshot
turn off laser









